{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIidirFbYRx_",
        "outputId": "129a7d61-8622-436e-d85e-55f88a161f99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ollama in /usr/local/lib/python3.11/dist-packages (0.4.7)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from ollama) (2.11.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.33.0)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.13.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install ollama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama if not installed (for Deepnote)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xUC6BTgZBu8",
        "outputId": "2b375364-80e0-4a78-8b9f-23d2f8c2053c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpC0k-4LZUXa",
        "outputId": "f53c9b88-2056-4c0b-ef7c-f9c25f994bf3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['ollama', 'serve']>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull mistral"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpWRfXtaZy_M",
        "outputId": "5cba77f8-af57-4d65-9213-0d16809559cb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling ff82381e2bea... 100% ‚ñï‚ñè 4.1 GB                         \u001b[K\n",
            "pulling 43070e2d4e53... 100% ‚ñï‚ñè  11 KB                         \u001b[K\n",
            "pulling 491dfa501e59... 100% ‚ñï‚ñè  801 B                         \u001b[K\n",
            "pulling ed11eda7790d... 100% ‚ñï‚ñè   30 B                         \u001b[K\n",
            "pulling 42347cd80dc8... 100% ‚ñï‚ñè  485 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ollama\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ‚úÖ Input and Output Paths\n",
        "csv_file = \"/content/drive/MyDrive/Colab Notebooks/chunks.csv\"  # Adjust the file path\n",
        "output_path = \"/content/drive/MyDrive/Colab Notebooks/chunks_paragraph_701_1000.csv\"\n",
        "\n",
        "# ‚úÖ Read CSV File Correctly\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# ‚úÖ Define start and end indices\n",
        "start_index = 701  # Adjust as needed\n",
        "end_index = 1000 # Adjust as needed\n",
        "\n",
        "# ‚úÖ Slice the dataset\n",
        "df = df.iloc[start_index:end_index].copy()  # Avoid SettingWithCopyWarning\n",
        "\n",
        "# ‚úÖ Function to paraphrase legal text\n",
        "def paraphrase_legal_text(text):\n",
        "    \"\"\"Generate a high-quality paraphrase of legal text while preserving its meaning.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"\"  # Skip empty text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Paraphrase the following legal text into a high-quality, well-structured version while maintaining its original meaning.\n",
        "    - Ensure clarity, conciseness, and legal accuracy.\n",
        "    - Use professional legal language without simplifying or omitting key details.\n",
        "\n",
        "    Text:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = ollama.chat(model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "        return response[\"message\"][\"content\"].strip()  # Return paraphrased text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error processing text: {e}\")\n",
        "        return text  # Return original text if an error occurs\n",
        "\n",
        "# ‚úÖ Apply Paraphrasing to the `chunked_text` Column\n",
        "df[\"Paraphrased_Text\"] = [paraphrase_legal_text(text) for text in tqdm(df[\"chunked_text\"], desc=\"üîÑ Paraphrasing Text\")]\n",
        "\n",
        "# ‚úÖ Save to CSV\n",
        "df.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(f\"‚úÖ Paraphrased text saved to '{output_path}' üéØ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlC0rd-lZ1hq",
        "outputId": "dfeb9a90-aaa8-4e89-b08e-5631c24626a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üîÑ Paraphrasing Text:   4%|‚ñé         | 11/299 [02:51<1:12:00, 15.00s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "np4xpI4ua5gc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}